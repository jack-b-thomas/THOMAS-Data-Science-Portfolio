{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IDS Week 14_2: Text Classification with Sentence Embeddings & Logistic Regression\n",
        "\n",
        "In this notebook, we’ll walk step-by-step through building a movie-review classifier on the Rotten Tomatoes dataset using pretrained sentence embeddings and scikit-learn’s Logistic Regression. We’ll cover:\n",
        "\n",
        "1. **Environment Setup** – install and import libraries.  \n",
        "2. **Load & Inspect Data** – load the Rotten Tomatoes reviews and take a first look.  \n",
        "3. **Generate Embeddings** – encode each review into a fixed-size vector via sentence-transformers.  \n",
        "4. **Train/Test Split** – partition our data for training and evaluation.  \n",
        "5. **Train Classifier** – fit a Logistic Regression model on embedding features.  \n",
        "6. **Evaluate Performance** – compute accuracy, confusion matrix, and classification report.  \n",
        "7. **Interpret Results** – inspect model coefficients and predicted probabilities.  "
      ],
      "metadata": {
        "id": "jAyA70ym3q5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Environment Setup**\n",
        "\n",
        "First, install the required libraries (`sentence-transformers` for embeddings, `datasets` for loading our data) and import everything we need."
      ],
      "metadata": {
        "id": "_nNnDm8h3u12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sentence-transformers for easy embeddings\n",
        "\n",
        "# Install the Hugging Face datasets library to load Rotten Tomatoes reviews\n"
      ],
      "metadata": {
        "id": "oDSyICy_Olz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Load & Inspect Data**\n",
        "We’ll load the [Cornell Movie Review “Rotten Tomatoes” dataset](https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes) (just the training split for sample size purposes) and convert it to a pandas DataFrame for easy inspection.\n",
        "- `label = 1` means \"fresh\"\n",
        "- `label = 0` means \"rotten\""
      ],
      "metadata": {
        "id": "50Js8z6h322y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A-_5xvyZt3p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Rotten Tomatoes dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution to ensure balance\n"
      ],
      "metadata": {
        "id": "HZNJJf9S4HKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Generate Sentence Embeddings**\n",
        "We use the [`SentenceTransformer` wrapper](https://huggingface.co/sentence-transformers), which exposes a simple `.encode()` method. It returns an (n_samples × embedding_dim) NumPy array."
      ],
      "metadata": {
        "id": "Jjpexoxf4T2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence-transformers for pretrained encoder\n",
        "\n",
        "# Instantiate the pretrained embedding model\n",
        "\n",
        "\n",
        "# Encode all review texts to embeddings\n",
        "# convert_to_numpy=True returns a NumPy array\n",
        "# show_progress_bar displays encoding progress\n",
        "\n",
        "# Inspect shape: (n_reviews, embedding_dimension)\n"
      ],
      "metadata": {
        "id": "vHAibjI1OtXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Split into Training & Testing Sets**\n",
        "We’ll hold out 20% of our embeddings/labels for testing, using a fixed random_state for reproducibility."
      ],
      "metadata": {
        "id": "8sgBdoph4i5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (embeddings) and target (labels)\n",
        "\n",
        "# Split embeddings and labels into train/test\n",
        "\n",
        "\n",
        "# Confirm sizes\n"
      ],
      "metadata": {
        "id": "mQiricbDP__8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Train Logistic Regression**\n",
        "Fit a logistic regression classifier on our training set. We use the default L2 penalty and solver, and let `scikit-learn` choose sensible defaults."
      ],
      "metadata": {
        "id": "p8YscAB24oKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "clf = LogisticRegression()\n",
        "\n",
        "# Train on embedding features\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HEneSS6jQwlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Evaluate Performance**\n",
        "We’ll predict on the test set, compute overall accuracy, plot the confusion matrix, and print a detailed classification report (precision, recall, F1-score)."
      ],
      "metadata": {
        "id": "Z3x3Dd3n5Irz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r3EigvoPSG1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Predict New Texts**\n",
        "Now that we have a trained model, let’s see how it does on user-supplied reviews. We’ll:\n",
        "\n",
        "1. Define a few custom review strings.  \n",
        "2. Encode them into embeddings.  \n",
        "3. Predict labels and class-probabilities with our logistic regressor.  \n",
        "4. Print out each review with its predicted sentiment and confidence."
      ],
      "metadata": {
        "id": "ZI6jGAhl5H9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.1 Define some new reviews to classify as a list\n",
        "\n",
        "\n",
        "# 7.2 Generate embeddings for these new texts\n",
        "# (using the same embedder we initialized earlier)\n",
        "\n",
        "\n",
        "# 7.3 Predict labels and probabilities\n",
        "\n",
        "\n",
        "# 7.4 Display results\n"
      ],
      "metadata": {
        "id": "pOlGAtPF5ZNG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}